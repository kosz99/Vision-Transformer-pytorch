{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageToPatchEmbeddings(nn.Module):\n",
    "    def __init__(self, latent_dim, patch_size):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.lin_projection = nn.Linear(3*self.patch_size*self.patch_size, self.latent_dim)\n",
    "\n",
    "        self.class_embedding = nn.Linear(self.latent_dim, self.latent_dim)\n",
    "\n",
    "        self.learnable_positional_enbedding = nn.Linear(self.latent_dim, self.latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unfold(-2, self.patch_size, self.patch_size)\n",
    "        x = x.unfold(-2, self.patch_size, self.patch_size)\n",
    "        x = x.movedim(1,-3)\n",
    "        x = x.flatten(1,2)\n",
    "        x = x.flatten(-3,-1)\n",
    "\n",
    "        x = self.lin_projection(x)\n",
    "\n",
    "        \n",
    "\n",
    "        pos = self.positions(x.shape[1], x.shape[2])\n",
    "\n",
    "        pos_embedding = self.learnable_positional_enbedding(pos)\n",
    "\n",
    "        x = x+pos_embedding\n",
    "\n",
    "        ones = torch.ones(x.shape[0], 1, self.latent_dim)\n",
    "        cls_embedding = self.class_embedding(ones)\n",
    "\n",
    "        embeddings = torch.cat((cls_embedding, x), 1)\n",
    "\n",
    "        return embeddings\n",
    "    \n",
    "    def positions(self, num_patch, latent_dim):\n",
    "        x = torch.ones(num_patch, latent_dim)\n",
    "        for i in range(num_patch):\n",
    "            x[i,:]*=i+1\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateQKV(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "\n",
    "        self.WQ = nn.Linear(d_model, d_model, bias= False)\n",
    "        self.WK = nn.Linear(d_model, d_model, bias= False)\n",
    "        self.WV = nn.Linear(d_model, d_model, bias= False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.WQ(x), self.WK(x), self.WV(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def Attention(query, key, values):\n",
    "        dk = query.size(1)\n",
    "        scores = nn.functional.softmax((torch.matmul(query, key.T)/np.sqrt(dk)), dim = 1)\n",
    "\n",
    "        return torch.matmul(scores, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.heads = heads\n",
    "\n",
    "        self.WQ = nn.ModuleList([nn.Linear(self.d_model,(self.d_model//self.heads), bias= False) for _ in range(self.heads)])\n",
    "        self.WK = nn.ModuleList([nn.Linear(self.d_model,(self.d_model//self.heads), bias= False) for _ in range(self.heads)])\n",
    "        self.WV = nn.ModuleList([nn.Linear(self.d_model,(self.d_model//self.heads), bias= False) for _ in range(self.heads)])\n",
    "        self.WO = nn.Linear(self.d_model, self.d_model, bias = False)\n",
    "    \n",
    "    def forward(self, query, key, values):\n",
    "        attn = []\n",
    "        for i in range(self.heads):\n",
    "            q = self.WQ[i](query)\n",
    "            k = self.WK[i](key)\n",
    "            v = self.WV[i](values)\n",
    "            \n",
    "            attn.append(Attention(q, k, v))\n",
    "        \n",
    "        cat_attn = torch.cat(attn, dim = 1)\n",
    "\n",
    "        return self.WO(cat_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, num_heads, latent_dim):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.layer_norm1 = nn.LayerNorm(self.latent_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(self.latent_dim)\n",
    "        self.qkv = CreateQKV(self.latent_dim)\n",
    "        self.MSA = MultiHeadAttention(self.latent_dim, self.num_heads)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_norm1 = self.layer_norm1(x)\n",
    "        q, k, v = self.qkv(x_norm)\n",
    "        attention = self.MSA(q, k, v)\n",
    "        add1 = x+attention\n",
    "        \n",
    "        \n",
    "        \n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = ImageToPatchEmbeddings(1024,16)\n",
    "o = pre(torch.rand(1,3,2048,2048))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16385, 1024])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.ones((5,10))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],\n",
       "       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n",
       "       [4., 4., 4., 4., 4., 4., 4., 4., 4., 4.],\n",
       "       [5., 5., 5., 5., 5., 5., 5., 5., 5., 5.]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(x.shape[0]):\n",
    "    x[i,:]*=i+1 \n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.97939543, 0.52654997, 0.68040251, 0.14814911, 0.66951159,\n",
       "         0.09927551, 0.56171187, 0.18610922, 0.37494385, 0.32788911],\n",
       "        [0.72699297, 0.96470595, 0.95027024, 0.17224075, 0.44855508,\n",
       "         0.51678971, 0.12351106, 0.32853366, 0.29950902, 0.63464192],\n",
       "        [0.21312251, 0.87399493, 0.9251385 , 0.00955824, 0.65094259,\n",
       "         0.88571189, 0.08502573, 0.93209916, 0.6730934 , 0.71931374],\n",
       "        [0.7492376 , 0.11851665, 0.63399055, 0.47007862, 0.30583137,\n",
       "         0.38315912, 0.82187785, 0.9527907 , 0.7735972 , 0.68588855],\n",
       "        [0.06134051, 0.86871411, 0.82104252, 0.04978584, 0.36772339,\n",
       "         0.97140903, 0.17883904, 0.11841364, 0.24477979, 0.72758525]],\n",
       "\n",
       "       [[0.73468369, 0.11631524, 0.97561519, 0.24343114, 0.79762096,\n",
       "         0.68914959, 0.84889841, 0.61958279, 0.3018582 , 0.66298499],\n",
       "        [0.27929298, 0.16951288, 0.53438094, 0.86954213, 0.46116236,\n",
       "         0.88433585, 0.59452318, 0.97853236, 0.19320634, 0.15282897],\n",
       "        [0.55038326, 0.57098554, 0.0759992 , 0.89105809, 0.95287128,\n",
       "         0.62866616, 0.25945669, 0.9422326 , 0.04982294, 0.01531764],\n",
       "        [0.41883415, 0.82581458, 0.64021339, 0.94504199, 0.34833001,\n",
       "         0.37071965, 0.98102938, 0.51064135, 0.05363603, 0.1615423 ],\n",
       "        [0.47307222, 0.28639061, 0.50504509, 0.93734961, 0.82645795,\n",
       "         0.14989359, 0.19479377, 0.79179171, 0.63827246, 0.26097791]],\n",
       "\n",
       "       [[0.77379193, 0.66066213, 0.03165881, 0.63439201, 0.62187172,\n",
       "         0.25271755, 0.15425799, 0.38098464, 0.84514072, 0.73563231],\n",
       "        [0.00929138, 0.5652692 , 0.82950679, 0.92073846, 0.26174512,\n",
       "         0.45313078, 0.73319925, 0.28312522, 0.70173681, 0.89044561],\n",
       "        [0.38573811, 0.87685061, 0.23089212, 0.30376025, 0.66431457,\n",
       "         0.39986001, 0.12765992, 0.23051845, 0.51738117, 0.25260959],\n",
       "        [0.26734332, 0.57069529, 0.09485937, 0.871118  , 0.21775878,\n",
       "         0.21284555, 0.13601419, 0.62243187, 0.93551556, 0.6851652 ],\n",
       "        [0.67393868, 0.91464467, 0.78892272, 0.84388431, 0.87160438,\n",
       "         0.98230363, 0.46445704, 0.38310157, 0.5742287 , 0.48629671]]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx = np.random.rand(3, 5, 10)\n",
    "xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1.97939543, 1.52654997, 1.68040251, 1.14814911, 1.66951159,\n",
       "         1.09927551, 1.56171187, 1.18610922, 1.37494385, 1.32788911],\n",
       "        [2.72699297, 2.96470595, 2.95027024, 2.17224075, 2.44855508,\n",
       "         2.51678971, 2.12351106, 2.32853366, 2.29950902, 2.63464192],\n",
       "        [3.21312251, 3.87399493, 3.9251385 , 3.00955824, 3.65094259,\n",
       "         3.88571189, 3.08502573, 3.93209916, 3.6730934 , 3.71931374],\n",
       "        [4.7492376 , 4.11851665, 4.63399055, 4.47007862, 4.30583137,\n",
       "         4.38315912, 4.82187785, 4.9527907 , 4.7735972 , 4.68588855],\n",
       "        [5.06134051, 5.86871411, 5.82104252, 5.04978584, 5.36772339,\n",
       "         5.97140903, 5.17883904, 5.11841364, 5.24477979, 5.72758525]],\n",
       "\n",
       "       [[1.73468369, 1.11631524, 1.97561519, 1.24343114, 1.79762096,\n",
       "         1.68914959, 1.84889841, 1.61958279, 1.3018582 , 1.66298499],\n",
       "        [2.27929298, 2.16951288, 2.53438094, 2.86954213, 2.46116236,\n",
       "         2.88433585, 2.59452318, 2.97853236, 2.19320634, 2.15282897],\n",
       "        [3.55038326, 3.57098554, 3.0759992 , 3.89105809, 3.95287128,\n",
       "         3.62866616, 3.25945669, 3.9422326 , 3.04982294, 3.01531764],\n",
       "        [4.41883415, 4.82581458, 4.64021339, 4.94504199, 4.34833001,\n",
       "         4.37071965, 4.98102938, 4.51064135, 4.05363603, 4.1615423 ],\n",
       "        [5.47307222, 5.28639061, 5.50504509, 5.93734961, 5.82645795,\n",
       "         5.14989359, 5.19479377, 5.79179171, 5.63827246, 5.26097791]],\n",
       "\n",
       "       [[1.77379193, 1.66066213, 1.03165881, 1.63439201, 1.62187172,\n",
       "         1.25271755, 1.15425799, 1.38098464, 1.84514072, 1.73563231],\n",
       "        [2.00929138, 2.5652692 , 2.82950679, 2.92073846, 2.26174512,\n",
       "         2.45313078, 2.73319925, 2.28312522, 2.70173681, 2.89044561],\n",
       "        [3.38573811, 3.87685061, 3.23089212, 3.30376025, 3.66431457,\n",
       "         3.39986001, 3.12765992, 3.23051845, 3.51738117, 3.25260959],\n",
       "        [4.26734332, 4.57069529, 4.09485937, 4.871118  , 4.21775878,\n",
       "         4.21284555, 4.13601419, 4.62243187, 4.93551556, 4.6851652 ],\n",
       "        [5.67393868, 5.91464467, 5.78892272, 5.84388431, 5.87160438,\n",
       "         5.98230363, 5.46445704, 5.38310157, 5.5742287 , 5.48629671]]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx+x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[[ 1.,  2.],\n",
       "           [ 5.,  6.]],\n",
       "\n",
       "          [[ 3.,  4.],\n",
       "           [ 7.,  8.]]],\n",
       "\n",
       "\n",
       "         [[[ 9., 10.],\n",
       "           [13., 14.]],\n",
       "\n",
       "          [[11., 12.],\n",
       "           [15., 16.]]]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_unfold.unfold(-2,2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.,  2.,  3.,  4.],\n",
       "         [ 5.,  6.,  7.,  8.],\n",
       "         [ 9., 10., 11., 12.],\n",
       "         [13., 14., 15., 16.]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "XX=X.unfold(-2, 2, 2)\n",
    "XX =XX.unfold(-2, 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.,  2.],\n",
       "          [ 5.,  6.]],\n",
       "\n",
       "         [[ 3.,  4.],\n",
       "          [ 7.,  8.]],\n",
       "\n",
       "         [[ 9., 10.],\n",
       "          [13., 14.]],\n",
       "\n",
       "         [[11., 12.],\n",
       "          [15., 16.]]]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XX.flatten(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
